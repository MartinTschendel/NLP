{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Bag of Words Classification Model<br>\n",
    "Positive Reviews: 1<br>\n",
    "Negative Reviews: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delimiter = \"\\t\" means tab\n",
    "#quoting = 3 means we are ignoring the double quotes in the dataset\n",
    "dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter='\\t', quoting = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Martin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#use vpn so that stopwords can be downloaded\n",
    "\n",
    "#importing text cleaning libraries\n",
    "import re \n",
    "#nltk helps to remove all useless words (stopwords) such as a, the. and ...\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "#stemming only concentrates on the roots of the word (loved --> love)\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the list corpus will contain all text cleaned reviews\n",
    "corpus = []\n",
    "#iterate through dataset\n",
    "for i in range(0, 1000):\n",
    "    #removing all punctuations\n",
    "    #meaning of [^a-zA-Z]: anything except...\n",
    "    #meaning of ' ': replacement by space\n",
    "    #meaning of dataset['Review'][i]: place where the cleaning step should happen \n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n",
    "    #transform capital letters to lower case by updating the 'review' variable\n",
    "    review = review.lower()\n",
    "    #split each review in different words, so that we can apply stemming\n",
    "    review = review.split()\n",
    "    #apply stemming (reduce the words to their roots)\n",
    "    #with 'ps' we created an object to apply stemming\n",
    "    ps = PorterStemmer()\n",
    "    #actually stopwords also includes the word 'not'\n",
    "    #but we have to keep it because it is a clearly negative indicator\n",
    "    all_stopwords = stopwords.words('english')\n",
    "    all_stopwords.remove('not')\n",
    "    #update 'review' by creating a list with the stemmed words\n",
    "    #this can be done with a single line for loop\n",
    "    #by applying ps.stem() to the iterator 'word'\n",
    "    #here we get also rid of the stopwords\n",
    "    review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n",
    "    #join the splitted words together in one review\n",
    "    #' ' means to separate the words with a space\n",
    "    review = ' '.join(review)\n",
    "    #putting each cleaned 'review' into the 'corpus' list \n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wow love place', 'crust not good', 'not tasti textur nasti', 'stop late may bank holiday rick steve recommend love', 'select menu great price', 'get angri want damn pho', 'honeslti tast fresh', 'potato like rubber could tell made ahead time kept warmer', 'fri great', 'great touch']\n"
     ]
    }
   ],
   "source": [
    "print(corpus[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create bag of words model\n",
    "#rows of sparse matrix: different reviews\n",
    "#columns of sparse matrix: cleaned words \n",
    "#cells of sparse matrix: 1 (word is in review) or 0 (word is not in review)\n",
    "#the above described matrix creation is done with tokenization\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#create an instance of the class 'CountVectorizer'\n",
    "#in () we have to enter the maximum number of words\n",
    "#we set this number after we know what is the number of the most frequent words\n",
    "cv = CountVectorizer()\n",
    "#'X' stands for our sparse matrix\n",
    "#'toarray()' transforms it to a 2D array\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "#create dependent vector y\n",
    "y = dataset.iloc[:, -1].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1566"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the number of columns (words) after the tocenization\n",
    "#[0] means first row\n",
    "len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we want to get rid of words that are not frequently used (the last 66 ones),\n",
    "#so we use 'max_features = 1500'\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "X = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset in training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.73\n"
     ]
    }
   ],
   "source": [
    "#predict test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "print('Accuracy', (y_test == y_pred).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[55 42]\n",
      " [12 91]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.73"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have 55 true negatives\n",
    "#91 true positives\n",
    "#42 false positives\n",
    "#12 false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', n_estimators=30, random_state=0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see if a random forest model can perform better\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "class_rf = RandomForestClassifier(n_estimators = 30, criterion = 'entropy', random_state = 0)\n",
    "class_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[90  7]\n",
      " [40 63]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.765"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = class_rf.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have 90 true negatives\n",
    "#63 true positives\n",
    "#7 false positives\n",
    "#40 false negatives"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
