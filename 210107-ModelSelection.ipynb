{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the performance of different classifiers by applying the following metrics:\n",
    "* Confusion Matrix\n",
    "* Accuracy (how many of the predicted results are similar to the test set results? \n",
    "* Precision (measuring exactness; when it predicts yes, how often is it correct?)\n",
    "* Recall (measuring completeness; when it's actually yes, how often does it predict yes?)\n",
    "* F1 Score (compromise between Precision and Recallï¼‰\n",
    "* Save the results within a dataframe and export it to a csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Martin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter='\\t', quoting = 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(0, 1000):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    all_stopwords = stopwords.words('english')\n",
    "    all_stopwords.remove('not')\n",
    "    review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n",
    "    review = ' '.join(review) \n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes Classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "def model_performance(y_pred):\n",
    "    value_list = []\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    TN = cm[0][0]\n",
    "    value_list.append(cm[0][0])\n",
    "    TP = cm[1][1]\n",
    "    value_list.append(cm[1][1])\n",
    "    FP = cm[0][1]\n",
    "    value_list.append(cm[0][1])\n",
    "    FN = cm[1][0]\n",
    "    value_list.append(cm[1][0])\n",
    "    Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    value_list.append(Accuracy)\n",
    "    Precision = TP / (TP + FP)\n",
    "    value_list.append(round(Precision, 3))\n",
    "    Recall = TP / (TP + FN)\n",
    "    value_list.append(round(Recall, 3))\n",
    "    F1 = 2 * Precision * Recall / (Precision + Recall)\n",
    "    value_list.append(round(F1, 3))\n",
    "    return print((cm),'\\n'\n",
    "    'True Negatives:', cm[0][0],'\\n'\n",
    "    'True Positives:', cm[1][1],'\\n'\n",
    "    'False Positives:', cm[0][1],'\\n'\n",
    "    'False Negatives:', cm[1][0],'\\n'\n",
    "    'Accurary:', Accuracy,'\\n'\n",
    "    'Precision:', round(Precision, 3),'\\n'\n",
    "    'Recall:', round(Recall, 3),'\\n'\n",
    "    'F1 Score:', round(F1, 3)), value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[55 42]\n",
      " [12 91]] \n",
      "True Negatives: 55 \n",
      "True Positives: 91 \n",
      "False Positives: 42 \n",
      "False Negatives: 12 \n",
      "Accurary: 0.73 \n",
      "Precision: 0.684 \n",
      "Recall: 0.883 \n",
      "F1 Score: 0.771\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[55, 91, 42, 12, 0.73, 0.684, 0.883, 0.771]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_nb = model_performance(y_pred)[1]\n",
    "list_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[90  7]\n",
      " [40 63]] \n",
      "True Negatives: 90 \n",
      "True Positives: 63 \n",
      "False Positives: 7 \n",
      "False Negatives: 40 \n",
      "Accurary: 0.765 \n",
      "Precision: 0.9 \n",
      "Recall: 0.612 \n",
      "F1 Score: 0.728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[90, 63, 7, 40, 0.765, 0.9, 0.612, 0.728]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "class_rf = RandomForestClassifier(n_estimators = 30, criterion = 'entropy', random_state = 0)\n",
    "class_rf.fit(X_train, y_train)\n",
    "y_pred = class_rf.predict(X_test)\n",
    "list_rf = model_performance(y_pred)[1]\n",
    "list_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[79 18]\n",
      " [24 79]] \n",
      "True Negatives: 79 \n",
      "True Positives: 79 \n",
      "False Positives: 18 \n",
      "False Negatives: 24 \n",
      "Accurary: 0.79 \n",
      "Precision: 0.814 \n",
      "Recall: 0.767 \n",
      "F1 Score: 0.79\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[79, 79, 18, 24, 0.79, 0.814, 0.767, 0.79]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Support Vector Machine Classifier\n",
    "from sklearn.svm import SVC\n",
    "class_svm = SVC(kernel = 'linear', random_state = 0)\n",
    "class_svm.fit(X_train, y_train)\n",
    "y_pred = class_svm.predict(X_test)\n",
    "list_svm = model_performance(y_pred)[1]\n",
    "list_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[89  8]\n",
      " [36 67]] \n",
      "True Negatives: 89 \n",
      "True Positives: 67 \n",
      "False Positives: 8 \n",
      "False Negatives: 36 \n",
      "Accurary: 0.78 \n",
      "Precision: 0.893 \n",
      "Recall: 0.65 \n",
      "F1 Score: 0.753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[89, 67, 8, 36, 0.78, 0.893, 0.65, 0.753]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Kernel SVM Classifier\n",
    "class_kern = SVC(kernel = 'rbf', random_state = 0)\n",
    "class_kern.fit(X_train, y_train)\n",
    "y_pred = class_kern.predict(X_test)\n",
    "list_kern = model_performance(y_pred)[1]\n",
    "list_kern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[80 17]\n",
      " [32 71]] \n",
      "True Negatives: 80 \n",
      "True Positives: 71 \n",
      "False Positives: 17 \n",
      "False Negatives: 32 \n",
      "Accurary: 0.755 \n",
      "Precision: 0.807 \n",
      "Recall: 0.689 \n",
      "F1 Score: 0.743\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[80, 71, 17, 32, 0.755, 0.807, 0.689, 0.743]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "class_tree = DecisionTreeClassifier(criterion = 'entropy', random_state=0)\n",
    "class_tree.fit(X_train, y_train)\n",
    "y_pred = class_tree.predict(X_test)\n",
    "list_tree = model_performance(y_pred)[1]\n",
    "list_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[88  9]\n",
      " [57 46]] \n",
      "True Negatives: 88 \n",
      "True Positives: 46 \n",
      "False Positives: 9 \n",
      "False Negatives: 57 \n",
      "Accurary: 0.67 \n",
      "Precision: 0.836 \n",
      "Recall: 0.447 \n",
      "F1 Score: 0.582\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[88, 46, 9, 57, 0.67, 0.836, 0.447, 0.582]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "class_knn = KNeighborsClassifier(n_neighbors = 10, metric = 'minkowski', p = 2)\n",
    "class_knn.fit(X_train, y_train)\n",
    "y_pred = class_knn.predict(X_test)\n",
    "list_knn = model_performance(y_pred)[1]\n",
    "list_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[80 17]\n",
      " [27 76]] \n",
      "True Negatives: 80 \n",
      "True Positives: 76 \n",
      "False Positives: 17 \n",
      "False Negatives: 27 \n",
      "Accurary: 0.78 \n",
      "Precision: 0.817 \n",
      "Recall: 0.738 \n",
      "F1 Score: 0.776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[80, 76, 17, 27, 0.78, 0.817, 0.738, 0.776]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "class_log = LogisticRegression(random_state = 0)\n",
    "class_log.fit(X_train, y_train)\n",
    "y_pred = class_log.predict(X_test)\n",
    "list_log = model_performance(y_pred)[1]\n",
    "list_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Naive Bayes</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>Linear SVM</th>\n",
       "      <th>Kernel SVM</th>\n",
       "      <th>Decision Tree</th>\n",
       "      <th>K-NN</th>\n",
       "      <th>Log Reg.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True Neg</th>\n",
       "      <td>55.000</td>\n",
       "      <td>90.000</td>\n",
       "      <td>79.000</td>\n",
       "      <td>89.000</td>\n",
       "      <td>80.000</td>\n",
       "      <td>88.000</td>\n",
       "      <td>80.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Pos</th>\n",
       "      <td>91.000</td>\n",
       "      <td>63.000</td>\n",
       "      <td>79.000</td>\n",
       "      <td>67.000</td>\n",
       "      <td>71.000</td>\n",
       "      <td>46.000</td>\n",
       "      <td>76.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Pos</th>\n",
       "      <td>42.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>18.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>17.000</td>\n",
       "      <td>9.000</td>\n",
       "      <td>17.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Neg</th>\n",
       "      <td>12.000</td>\n",
       "      <td>40.000</td>\n",
       "      <td>24.000</td>\n",
       "      <td>36.000</td>\n",
       "      <td>32.000</td>\n",
       "      <td>57.000</td>\n",
       "      <td>27.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.730</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.684</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.814</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.807</td>\n",
       "      <td>0.836</td>\n",
       "      <td>0.817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.883</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.689</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.771</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.743</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Naive Bayes  Random Forest  Linear SVM  Kernel SVM  Decision Tree  \\\n",
       "True Neg        55.000         90.000      79.000      89.000         80.000   \n",
       "True Pos        91.000         63.000      79.000      67.000         71.000   \n",
       "False Pos       42.000          7.000      18.000       8.000         17.000   \n",
       "False Neg       12.000         40.000      24.000      36.000         32.000   \n",
       "Accuracy         0.730          0.765       0.790       0.780          0.755   \n",
       "Precision        0.684          0.900       0.814       0.893          0.807   \n",
       "Recall           0.883          0.612       0.767       0.650          0.689   \n",
       "F1 Score         0.771          0.728       0.790       0.753          0.743   \n",
       "\n",
       "             K-NN  Log Reg.  \n",
       "True Neg   88.000    80.000  \n",
       "True Pos   46.000    76.000  \n",
       "False Pos   9.000    17.000  \n",
       "False Neg  57.000    27.000  \n",
       "Accuracy    0.670     0.780  \n",
       "Precision   0.836     0.817  \n",
       "Recall      0.447     0.738  \n",
       "F1 Score    0.582     0.776  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(zip(list_nb, list_rf, list_svm, list_kern, list_tree, list_knn, list_log), index = ['True Neg', 'True Pos', 'False Pos',\n",
    "                                    'False Neg', 'Accuracy', 'Precision',\n",
    "                                    'Recall', 'F1 Score'], columns = ['Naive Bayes', 'Random Forest', 'Linear SVM', \n",
    "                                                                      'Kernel SVM', 'Decision Tree', 'K-NN', 'Log Reg.'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('model_selection.csv', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
